# Llama Manager Configuration
# Copy this file to .env and modify as needed

# API server port (serves the web UI)
API_PORT=3001

# Llama.cpp server port (OpenAI-compatible API)
LLAMA_PORT=8080

# Directory to store models
MODELS_DIR=~/models

# Maximum number of models loaded simultaneously
MODELS_MAX=2

# Default context size
CONTEXT_SIZE=8192

# Distrobox container name to use for starting llama.cpp
# Set this to your distrobox container name, e.g. 'llama'
DISTROBOX_CONTAINER=llama-rocm-7rc-rocwmma

# Auto-start llama server when API starts
AUTO_START=true

# WebSocket stats update interval in milliseconds
STATS_INTERVAL=1000

# Override the llama.cpp UI URL (useful if running behind a reverse proxy)
# If not set, defaults to http://<current-hostname>:<LLAMA_PORT>
# LLAMA_UI_URL=http://llama.example.com

# HuggingFace token for downloading gated/private models
# Get your token from: https://huggingface.co/settings/tokens
# HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxx
